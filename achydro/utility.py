import csv
import configparser
import pandas as pd
import geopandas as gpd
from pathlib import Path
from osgeo import gdal
import icepyx as ipx
import io
from fiona.io import ZipMemoryFile
import subprocess

import dask 
import dask.dataframe as dd
from dask.distributed import Client as DaskClient
from dask.diagnostics import ProgressBar

import numpy as np
from scipy.stats import median_abs_deviation
from sklearn.gaussian_process.kernels import ConstantKernel
from sklearn.gaussian_process.kernels import RationalQuadratic

from achydro.atl06lib import read_atl06, read_h5
from carst.libdhdt import DemPile, sigmoid_reg, gp_reg, wl_reg
from carst.libraster import SingleRaster

def clip_extent(gdf, poly):
    """
    Clip (select) polygons in the `gdf` data by a `poly` shape.
    gdf: GeoDataFrame, multi-polygon.
    poly: polygon geometry. Generated by gdf2.geometry.unary_union
    """
    gdf_sub = gdf.iloc[gdf.sindex.query(poly, predicate="intersects")]
    clipped = gdf_sub.copy()
    clipped["geometry"] = gdf_sub.intersection(poly)
    return clipped

def get_buffered_polygeo_bounds(aoi_polygon_file, buffer_size):
    gdf_refgeo = gpd.read_file(aoi_polygon_file)
    poly_refgeo = gdf_refgeo.geometry.unary_union
    x1, y1, x2, y2 = poly_refgeo.bounds
    x1 -= buffer_size
    y1 -= buffer_size
    x2 += buffer_size
    y2 += buffer_size
    return x1, y1, x2, y2

def mean_smallest75percent(a : np.array):
    threshold = np.percentile(a, 75)
    answer = a[a < threshold].mean()
    return answer

def std_outlierremoved(a : np.array):
    """
    See Zheng et al. (2018)
    """
    threshold_low  = np.median(a) - 3 * median_abs_deviation(a)
    threshold_high = np.median(a) + 3 * median_abs_deviation(a)
    filtered = a[a > threshold_low]
    filtered = filtered[filtered < threshold_high]
    answer = np.std(filtered)
    return answer


def wlr_get_event_slope(a, eps=20, min_samples=3, min_datapoints=4, min_time_span=365):
    """
    For linear fitting.
    a: carst.PixelTimeSeries object
    """
    xx = a.get_date()
    yy = a.get_value()
    ye = a.get_uncertainty()
    # ====
    # good_idx = a.bitmask_labels == 0
    exitstate, evmd_labels = a.do_evmd(eps=eps, min_samples=min_samples)
    good_idx = np.logical_and(a.bitmask_labels == 0, evmd_labels >= 0)
    # ====
    if not np.any(good_idx):
        slope = np.nan
        duration = np.nan
        return slope, duration
    
    xx_good = xx[good_idx]
    yy_good = yy[good_idx]
    ye_good = ye[good_idx]

    x_pred, y_pred, slope, slope_stderr, duration, exitstate = wl_reg(
        xx_good, yy_good, ye=ye_good, min_datapoints=min_datapoints, min_time_span=min_time_span)
    
    return slope, duration


def sigmoid_get_event_dh_timing(a, eps=20, min_samples=3, k_bounds=None, x0_bounds=None, downward_first=True):
    """
    For sigmoid fitting.
    a: carst.PixelTimeSeries object
    """
    xx = a.get_date()
    yy = a.get_value()
    ye = a.get_uncertainty()
    # ====
    # good_idx = a.bitmask_labels == 0
    exitstate, evmd_labels = a.do_evmd(eps=eps, min_samples=min_samples)
    good_idx = np.logical_and(a.bitmask_labels == 0, evmd_labels >= 0)
    # ====
    if not np.any(good_idx):
        sigmoid_height = np.nan
        sigmoid_timing = np.nan
        return sigmoid_height, sigmoid_timing
    
    xx_good = xx[good_idx]
    yy_good = yy[good_idx]
    ye_good = ye[good_idx]
    
    x_pred, y_pred, sigmoid_height, sigmoid_height_stderr, sigmoid_timing, exitstate = sigmoid_reg(
        xx_good, yy_good, ye=ye_good, k_bounds=k_bounds, x0_bounds=x0_bounds, downward_first=downward_first)

    return sigmoid_height, sigmoid_timing


def gp_get_event_dh_timing(a, eps=20, min_samples=3, gp_kernel=None):
    """
    For Gaussian Process fitting.
    a: carst.PixelTimeSeries object
    """
    xx = a.get_date()
    yy = a.get_value()
    ye = a.get_uncertainty()
    # ====
    # good_idx = a.bitmask_labels == 0
    exitstate, evmd_labels = a.do_evmd(eps=eps, min_samples=min_samples)
    good_idx = np.logical_and(a.bitmask_labels == 0, evmd_labels >= 0)
    # ====
    if not np.any(good_idx) or np.sum(good_idx) <= 3:
        max_transient_dh = np.nan
        max_transient_timing = np.nan
        return max_transient_dh, max_transient_timing
    
    xx_good = xx[good_idx]
    yy_good = yy[good_idx]
    ye_good = ye[good_idx]

    if gp_kernel is None:
        gp_kernel = ConstantKernel(constant_value=160, constant_value_bounds='fixed') * RationalQuadratic(
                                   length_scale=1.2, alpha=0.1, alpha_bounds='fixed', length_scale_bounds='fixed')

    x_pred_pos, y_prediction, max_transient_dh, max_transient_dh_stderr, max_transient_timing, exitstate = gp_reg(
        xx_good, yy_good, ye=ye_good, kernel=gp_kernel)

    
    return max_transient_dh, max_transient_timing



def read_zipped_shapefile(zipped_shapefile, working_epsg):
    zipshp = io.BytesIO(open(zipped_shapefile, 'rb').read())
    with (ZipMemoryFile(zipshp)) as memfile:
        with memfile.open() as src:
            crs = src.crs
            gdf = gpd.GeoDataFrame.from_features(src, crs=crs)
    gdf = gdf.to_crs(epsg=working_epsg)
    return gdf


def acquire_dem_list(arcticdem_index_file: str, 
                     aoi_polygon_file: str, 
                     output_wishlist_file: str):
    """
    AC-hydro Step 1. Acquire DEM list.
    """
    gdf_refgeo = gpd.read_file(aoi_polygon_file)
    poly_refgeo = gdf_refgeo.geometry.unary_union
    gdf_arcticdem = gpd.read_file(arcticdem_index_file)
    geom_intersection = clip_extent(gdf_arcticdem, poly_refgeo)

    with open(output_wishlist_file, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['filename', 'date', 'uncertainty'])

        for s3url in geom_intersection.s3url:
            dem_url = s3url.replace('https://polargeospatialcenter.github.io/stac-browser/#/external/', 
                                    '/vsicurl/https://') # Specify COG driver
            dem_url = dem_url.replace('.json', '_dem.tif')
            date = dem_url.split('_')[3]  # yyyymmdd
            date = date[:4] + '-' + date[4:6] + '-' + date[6:]  # yyyy-mm-dd
            uncertainty = '4'
            writer.writerow([dem_url, date, uncertainty])

def download_subset_dems(wishlist_file: str, 
                         dem_subsets_folder: str, 
                         bitmask_subsets_folder: str, 
                         aoi_polygon_file: str, 
                         buffer_size: float):
    """
    AC-hydro Step 2. Download Subset DEMs.
    """

    Path(dem_subsets_folder).mkdir(exist_ok=True)
    Path(bitmask_subsets_folder).mkdir(exist_ok=True)

    # Add a clip buffer size (for reclipping after ICEsat-2 alignment is done)
    # gdf_refgeo = gpd.read_file(aoi_polygon_file)
    # poly_refgeo = gdf_refgeo.geometry.unary_union
    x1, y1, x2, y2 = get_buffered_polygeo_bounds(aoi_polygon_file, buffer_size)
    # x1, y1, x2, y2 = poly_refgeo.bounds
    # x1 -= buffer_size
    # y1 -= buffer_size
    # x2 += buffer_size
    # y2 += buffer_size

    with open(wishlist_file, newline='') as f:
        csvcontent = csv.reader(f, skipinitialspace=True)
        next(csvcontent, None)    # Skip the header
        for row in csvcontent:
            cog_path = row[0]
            ds = gdal.Open(cog_path)
            original_filename = Path(cog_path).name
            ds = gdal.Translate(f'{dem_subsets_folder}/processed-{original_filename}', ds, projWin = [x1, y2, x2, y1])
            ds = None
            bitmask_cog_path = cog_path.replace('dem.tif', 'bitmask.tif')
            ds = gdal.Open(bitmask_cog_path)
            original_filename = Path(bitmask_cog_path).name
            ds = gdal.Translate(f'{bitmask_subsets_folder}/processed-{original_filename}', ds, projWin = [x1, y2, x2, y1])
            ds = None

def download_is2_atl6_data(aoi_polygon_file: str,
                           altimetry_subsets_folder: str, 
                           altimetry_acq_time=['2018-01-01','2022-01-01'],
                           altimetry_acq_cycles=['03', '04', '07', '08', '11', '12'],
                           altimetry_acq_version='006'):
    """
    AC-hydro Step 3. Acquire ICESat-2 ATL-6 points.
    """
    Path(altimetry_subsets_folder).mkdir(exist_ok=True)

    gdf_refgeo = gpd.read_file(aoi_polygon_file)
    refgeo_latlon = gdf_refgeo.to_crs(epsg=4326)
    refgeo_poly_latlon = refgeo_latlon.geometry.unary_union
    xx, yy = refgeo_poly_latlon.exterior.coords.xy
    refgeo_poly_vertices = np.column_stack((xx, yy))

    region_a = ipx.Query("ATL06", 
                         spatial_extent=refgeo_poly_vertices, 
                         date_range=altimetry_acq_time, 
                         cycles=altimetry_acq_cycles, 
                         version=altimetry_acq_version)
    print(region_a.avail_granules())
    region_a.download_granules(altimetry_subsets_folder)


def filter_is2_atl6_data(altimetry_subsets_folder: str,
                         altimetry_processed_folder: str,
                         rgi_index_file: str,
                         aoi_polygon_file: str,
                         working_epsg: int,
                         off_ice_buffer: float,
                         output_selected_elev_file: str,
                         if_additional_exclusion: bool=False,
                         additional_exclusion_file: str=None):
                          
    """
    AC-hydro Step 4. Select ICESat-2 ATL-6 points over good bedrock.
    Using dask dataframe.
    """

    alt_raw_path = Path(altimetry_subsets_folder)
    alt_processed_path = Path(altimetry_processed_folder)
    alt_processed_path.mkdir(exist_ok=True)
    gdf_refgeo = gpd.read_file(aoi_polygon_file)
    poly_refgeo = gdf_refgeo.geometry.unary_union

    files = list(alt_raw_path.glob('*.h5'))
    for f in files:
        read_atl06(f, epsg=working_epsg, outdir=altimetry_processed_folder, bbox=None)
    
    fin_files = list(alt_processed_path.glob('*.h5'))
    
    dfs = []
    for f in fin_files:
        f_data, vnames = read_h5(f)
        dd_data = dd.from_array(f_data, columns=vnames)
        dfs.append(dd_data)

    # Single parallel dataframe (larger than memory)
    df = dd.concat(dfs)

    # dask dataframe convert to pandas dataframe
    df_atl06 = df.compute()
    # print('Number of points:', len(df_atl06))

    
    
    # zipshp = io.BytesIO(open(rgi_index_file, 'rb').read())

    # with (ZipMemoryFile(zipshp)) as memfile:
    #     with memfile.open() as src:
    #         crs = src.crs
    #         gdf_rgi = gpd.GeoDataFrame.from_features(src, crs=crs)

    # gdf_rgi = gdf_rgi.to_crs(epsg=working_epsg)
    gdf_rgi = read_zipped_shapefile(rgi_index_file, working_epsg)
    try:
        gdf_rgi_summary_polygon = gdf_rgi.geometry.unary_union
    except:    # GEOSException: TopologyException for RGI 7 file
        valid_geom_idx = np.where(gdf_rgi.geometry.is_valid)
        gdf_rgi_summary_polygon = gdf_rgi.iloc[valid_geom_idx].geometry.unary_union


    icesat2_searching_area = poly_refgeo.difference(gdf_rgi_summary_polygon.buffer(off_ice_buffer))

    # Exclude specified area
    if if_additional_exclusion:
        additional_exclusion_refgeo = gpd.read_file(additional_exclusion_file)
        additional_exclusion_poly = additional_exclusion_refgeo.geometry.unary_union
        icesat2_searching_area = icesat2_searching_area.difference(additional_exclusion_poly)
    
    gdf_atl06 = gpd.GeoDataFrame(df_atl06, geometry=gpd.points_from_xy(df_atl06['x'], df_atl06['y']), crs=f'EPSG:{working_epsg}')
    pt_gs = gpd.GeoSeries(gdf_atl06.geometry)
    office_idx = pt_gs.within(icesat2_searching_area)
    gdf_office = gdf_atl06.iloc[np.where(office_idx)]

    ref_h = np.column_stack((gdf_office['x'], gdf_office['y'], gdf_office['h_li']))
    np.savetxt(output_selected_elev_file, ref_h, delimiter = ",", header="x,y,h_li", comments="")


def align_dems(aoi_polygon_file: str,
               buffer_size: float,
               selected_elev_file: str,
               dem_subsets_folder: str,
               dem_processed_folder: str,
               bitmask_subsets_folder: str):
    """
    AC-hydro Step 5. Align every DEMs with ICESat-2 reference points, and calcuate uncertainty.
    """

    x1, y1, x2, y2 = get_buffered_polygeo_bounds(aoi_polygon_file, buffer_size)
            
    with open('projwin.tmp', 'w') as f:
        f.write(f"{x1+1} {y1+1} {x2-1} {y2-1}")

    bashloc = Path(__file__).parent / 'asp_align.bash'
    subprocess.run(['bash', 
                    str(bashloc), 
                    selected_elev_file, 
                    dem_subsets_folder, 
                    dem_processed_folder, 
                    bitmask_subsets_folder], check=True)

def make_dem_manifest(dem_summary_file: str,
                      dem_processed_folder: str,
                      dem_input_processed_file: str):
    """
    AC-hydro Step 6. Analyze the processed DEMs and make manifest.
    """
    dem_processed_path = Path(dem_processed_folder)
    dem_processed_path.mkdir(exist_ok=True)
    
    with open(dem_summary_file, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['filename', 'date', 'trans-x', 'trans-y', 'trans-z', 'beginning-error', 'end-error', 'uncertainty'])
        
        for i in sorted(dem_processed_path.glob('*bitmask.tif'), key=lambda s: s.as_posix().split("_")[4]):
            prefix = i.as_posix().replace('_bitmask.tif', '')
            datestr = prefix.split("_")[4]
            datestr = f"{datestr[:4]}-{datestr[4:6]}-{datestr[6:]}"

            if Path(f"{prefix}_dem.tif").is_file():
                vectorfile = f"{prefix}-inverse-transform.txt"
                vector_matrix = np.loadtxt(vectorfile)
                vector_translation = vector_matrix[:-1, -1]
                dx, dy, dz = vector_translation
                # print(vector_translation)
                begerrorfile = f"{prefix}-beg_errors.csv"
                # print(begerrorfile)
                begerror_matrix = np.loadtxt(begerrorfile, delimiter=',')
                if begerror_matrix.shape[0] <= 4:    # Skip DEMs with ICESat measurements <= 4
                    continue
                begerror_values = begerror_matrix[:, -1]
                beg_deviation = mean_smallest75percent(begerror_values)
                # print(beg_deviation)
                enderrorfile = f"{prefix}-end_errors.csv"
                enderror_matrix = np.loadtxt(enderrorfile, delimiter=',')
                enderror_values = enderror_matrix[:, -1]
                end_deviation = mean_smallest75percent(enderror_values)
                dem_error = std_outlierremoved(enderror_values)
                # print(end_deviation)
                # print(dem_error)
                writer.writerow([f"{prefix}_dem.tif", datestr, dx,    dy,    dz,    beg_deviation, end_deviation, dem_error])
            else:
                writer.writerow([f"{prefix}_dem.tif", datestr, "NaN", "NaN", "NaN", "NaN", "NaN", "NaN"])
                
    df_processeddem = pd.read_csv(dem_summary_file)
    df_processeddem.drop(index=df_processeddem[np.isnan(df_processeddem['trans-x'])].index, 
                         columns=['trans-x', 'trans-y', 'trans-z', 'beginning-error', 'end-error'], 
                         inplace=True)
    df_processeddem.to_csv(dem_input_processed_file, index=False)

def stack_dems(tag: str,
               refgeo_gtif: str,
               carst_inipath: str,
               dem_input_processed_file: str,
               refdate: "datetime object",
               evmd_threshold: int):
    """
    AC-hydro Step 7. Stack all DEMs.
    """
    picklefile = f'refgeo-{tag}_15m_TSpickle.p'
    dhdt_prefix = f'{tag}_15m'
    
    refgeo_gtif_path = Path(refgeo_gtif)
    if not refgeo_gtif_path.is_file():
        subprocess.run(['gdal_rasterize', 
                        '-l', f'refgeo-{tag}',
                        '-tap', 
                        '-burn', '1.0',
                        '-tr', '15.0', '15.0',
                        '-a_nodata', '-9999.0',
                        '-ot', 'Float32',
                        '-of', 'GTiff',
                        f'refgeo-{tag}.gpkg', refgeo_gtif], check=True)

    # make configuration file
    if not Path(carst_inipath).is_file():
        config = configparser.ConfigParser()

        config.add_section('demlist')
        config.set('demlist', 'csvfile', dem_input_processed_file)
        config.add_section('refgeometry')
        config.set('refgeometry', 'gtiff', refgeo_gtif)
        config.add_section('settings')
        config.set('settings', 'refdate', refdate.strftime('%Y-%m-%d'))
        config.set('settings', 'max_uncertainty', '20')    # Tentative
        config.set('settings', 'min_time_span', '365')     # Tentative
        config.add_section('regression')
        config.set('regression', 'evmd_threshold', str(evmd_threshold))
        config.add_section('result')
        config.set('result', 'picklefile', picklefile)
        config.set('result', 'dhdt_prefix', dhdt_prefix)

        with open(carst_inipath, 'w') as configfile:
            config.write(configfile) 
            
    pile = DemPile()
    pile.read_config(carst_inipath)
    pile.init_ts()
    pile.pileup(bitmask=True)
    pile.dump_pickle()
    return pile


def dhdt_fit_sigmoid(pile: "DemPile object",
                     tag: str,
                     final_output_tag: str,
                     nproc: int=32,
                     chunksize: "2-tuple"=(100, 100),
                     evmd_threshold: int=20,
                     min_samples: int=3,
                     k_bounds: "2-tuple"=[10, 150],
                     downward_first: bool=True):
    """
    AC-hydro Step 9. Fit elevation change with a sigmoid model. 
    
    nproc: number of processors
    skip x0_bounds for now
    """
    dhdt_prefix = f'{tag}_15m'
    
    client = DaskClient(n_workers=nproc)
    
    drain_dh_map = np.full_like(pile.ts, np.nan, dtype=float)
    drain_t_map =  np.full_like(pile.ts, np.nan, dtype=float)
    
    # x0_bounds = [0, 3]  # roughly 2019 -- 2022
    
    def batch(seq, evmd_threshold, min_samples, k_bounds): # , x0_bounds):
        sub_results = []
        for x in seq:
            # sigmoid_height, sigmoid_timing = sigmoid_get_event_dh_timing(
            #     x, eps=evmd_threshold, min_samples=min_samples, k_bounds=k_bounds, x0_bounds=x0_bounds)
            sigmoid_height, sigmoid_timing = sigmoid_get_event_dh_timing(x, eps=evmd_threshold, min_samples=min_samples, k_bounds=k_bounds, downward_first=downward_first)
            sub_results.append((sigmoid_height, sigmoid_timing))
        return sub_results
    
    msize = chunksize[0]
    nsize = chunksize[1]
    m_nodes = np.arange(0, pile.ts.shape[0], msize)
    n_nodes = np.arange(0, pile.ts.shape[1], nsize)
    super_results = []
    
    for super_m in range(m_nodes.size):
        pile.display_progress(m_nodes[super_m], pile.ts.shape[0])
        batches = []
        ts_slice = pile.ts[m_nodes[super_m]:m_nodes[super_m]+msize, :]
        for m in range(ts_slice.shape[0]):
            for n in range(n_nodes.size):
                # result_batch = dask.delayed(batch)(ts_slice[m, n_nodes[n]:n_nodes[n]+nsize ], 
                #                                    evmd_threshold, min_samples, k_bounds, x0_bounds)
                result_batch = dask.delayed(batch)(ts_slice[m, n_nodes[n]:n_nodes[n]+nsize ], evmd_threshold, min_samples, k_bounds)
                batches.append(result_batch)
                
        with ProgressBar():
            results = dask.compute(batches)
        super_results.append(results[0])
    
    for m in range(pile.ts.shape[0]):
        for n in range(pile.ts.shape[1]):
            idx1 = m // msize
            idx2 = n_nodes.size * (m % msize) + n // nsize
            idx3 = n % nsize
            sigmoid_height = super_results[idx1][idx2][idx3][0]
            sigmoid_timing = super_results[idx1][idx2][idx3][1]
            drain_dh_map[m, n] = sigmoid_height
            drain_t_map[m, n] = sigmoid_timing
            
    if client is not None:
        client.close()
        
    drain_dh_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_dh.tif")
    drain_t_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_t.tif")
    
    drain_dh_map_raster.Array2Raster(drain_dh_map, pile.refgeo)
    drain_t_map_raster.Array2Raster(drain_t_map, pile.refgeo)
    
    return drain_dh_map, drain_t_map





def dhdt_fit_wl(pile: "DemPile object",
                tag: str,
                final_output_tag: str,
                nproc: int=32,
                chunksize: "2-tuple"=(100, 100),
                evmd_threshold: int=20,
                min_samples: int=3,
                min_datapoints: int=4, 
                min_time_span: float=365):
    """
    AC-hydro Step 9 (alternative). Fit elevation change with a weighted linear model.
    
    nproc: number of processors
    """
    dhdt_prefix = f'{tag}_15m'
    
    client = DaskClient(n_workers=nproc)
    
    drain_slope_map    = np.full_like(pile.ts, np.nan, dtype=float)
    drain_duration_map = np.full_like(pile.ts, np.nan, dtype=float)
    
    def batch(seq, evmd_threshold, min_samples, min_datapoints, min_time_span):
        sub_results = []
        for x in seq:
            slope, duration = wlr_get_event_slope(x, eps=evmd_threshold, min_samples=min_samples, min_datapoints=min_datapoints, min_time_span=min_time_span)
            sub_results.append((slope, duration))
        return sub_results
    
    msize = chunksize[0]
    nsize = chunksize[1]
    m_nodes = np.arange(0, pile.ts.shape[0], msize)
    n_nodes = np.arange(0, pile.ts.shape[1], nsize)
    super_results = []
    
    for super_m in range(m_nodes.size):
        pile.display_progress(m_nodes[super_m], pile.ts.shape[0])
        batches = []
        ts_slice = pile.ts[m_nodes[super_m]:m_nodes[super_m]+msize, :]
        for m in range(ts_slice.shape[0]):
            for n in range(n_nodes.size):
                result_batch = dask.delayed(batch)(ts_slice[m, n_nodes[n]:n_nodes[n]+nsize ], evmd_threshold, min_samples, min_datapoints, min_time_span)
                batches.append(result_batch)
                
        with ProgressBar():
            results = dask.compute(batches)
        super_results.append(results[0])
    
    for m in range(pile.ts.shape[0]):
        for n in range(pile.ts.shape[1]):
            idx1 = m // msize
            idx2 = n_nodes.size * (m % msize) + n // nsize
            idx3 = n % nsize
            slope    = super_results[idx1][idx2][idx3][0]
            duration = super_results[idx1][idx2][idx3][1]
            drain_slope_map[m, n] = slope
            drain_duration_map[m, n] = duration
            
    if client is not None:
        client.close()
        
    drain_slope_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_slope.tif")
    drain_duration_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_duration.tif")
    
    drain_slope_map_raster.Array2Raster(drain_slope_map, pile.refgeo)
    drain_duration_map_raster.Array2Raster(drain_duration_map, pile.refgeo)
    
    return drain_slope_map, drain_duration_map




def dhdt_fit_gp(pile: "DemPile object",
                tag: str,
                final_output_tag: str,
                evmd_threshold: int=20,
                min_samples: int=3,
                gp_kernel: "Kernel instance"=ConstantKernel(constant_value=160, constant_value_bounds='fixed') * RationalQuadratic(
                                       length_scale=1.2, alpha=0.1, alpha_bounds='fixed', length_scale_bounds='fixed')
                ):
    """
    AC-hydro Step 9 (alternative). Fit elevation change with a sigmoid model. 
    """

    dhdt_prefix = f'{tag}_15m'
    drain_dh_map = np.full_like(pile.ts, np.nan, dtype=float)
    drain_t_map =  np.full_like(pile.ts, np.nan, dtype=float)

    for m in range(pile.ts.shape[0]):
        pile.display_progress(m, pile.ts.shape[0])
        for n in range(pile.ts.shape[1]):
            max_transient_dh, max_transient_timing = gp_get_event_dh_timing(pile.ts[m, n], eps=evmd_threshold, min_samples=min_samples, gp_kernel=gp_kernel)
            drain_dh_map[m, n] = max_transient_dh
            drain_t_map[m, n] = max_transient_timing

    drain_dh_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_gp-dh.tif")
    drain_t_map_raster = SingleRaster(f"{dhdt_prefix}_{final_output_tag}_gp-t.tif")
    
    drain_dh_map_raster.Array2Raster(drain_dh_map, pile.refgeo)
    drain_t_map_raster.Array2Raster(drain_t_map, pile.refgeo)
    
    return drain_dh_map, drain_t_map
    
